<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>notes on diffusion models</title>

    <script src="https://unpkg.com/@tailwindcss/browser@4"></script>
    <script src="https://unpkg.com/lucide@latest"></script>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <script>
      document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false }
          ]
        });
      });
    </script>
  </head>

  <body class="bg-orange-50 text-gray-900 w-full max-w-3xl mx-auto px-5 md:px-0 pb-20">

    <header class="mt-12">
      <h1 class="font-serif text-4xl font-bold leading-tight text-gray-950">
        Diffusion models — Langevin & SDE
      </h1>
      <p class="text-sm text-gray-600 mt-4 italic">
        connecting langevin diffusion, SDEs, and diffusion models • jan 2026
      </p>
    </header>

    <article class="mt-10 space-y-10 text-lg text-gray-800 leading-relaxed font-serif">

      <section class="space-y-4">
        <p>
          There are basically some sections i want to connect & establish a link btw them.
          First of them being langevin diffusion, see diffusion model rely on Stochastic differential equations & langevin is a specific SDE which is fundamental to understand diffusion.
          Another one is Ito-SDE we’ll cover it too.
        </p>
      </section>

      <section class="space-y-4">
        <h2 class="font-bold text-xl">What is Langevin diffusion??</h2>

        <p>
          Langevin diffusion has been discovered in physics to describe the motion of particles driven by random and deterministic forces.
          Due to the random forces, it is a <strong>stochastic process</strong> that in generative AI describes the evolution of a probability distribution over time.
        </p>

        <p>
          Langevin diffusion is often used as a way to sample from a probability distribution p(x).
          Think of p(x) as a very complex high dimensional distribution that represents a dataset.
        </p>
      </section>

      <section class="space-y-4">
        <p>Gaussian mixture is given by a weighted sum of Gaussian distributions:</p>

        <p>
          $$
          p(\mathbf{x}) = \sum\limits_{j=1}^{n}q_j \mathcal{N}(\mathbf{x};\mu_i,\Sigma_{i}), \quad
          \text{where } q_j\in\mathbb{R},\sum\limits_{j=1}^{n}q_j = 1,
          \mu_i\in\mathbb{R}^2,\Sigma_{i}\in\mathbb{R}^{2\times 2}
          $$
        </p>

        <p>
          namely the density, log-density, score (gradient of log_density), and gradient of the density.
        </p>

        <p>
          $$
          \begin{aligned}
          \mathcal{N}(x;\mu,\Sigma)
          &= \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}
          \exp\!\left(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu)\right), \\[6pt]

          \log \mathcal{N}(x;\mu,\Sigma)
          &= -\frac{d}{2}\log(2\pi)
          - \frac{1}{2}\log|\Sigma|
          - \frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu), \\[6pt]

          \nabla_x \log \mathcal{N}(x;\mu,\Sigma)
          &= -\Sigma^{-1}(x-\mu), \\[6pt]

          \nabla_x \mathcal{N}(x;\mu,\Sigma)
          &= -\mathcal{N}(x;\mu,\Sigma)\,\Sigma^{-1}(x-\mu).
          \end{aligned}
          $$
        </p>
      </section>

      <section class="space-y-4">
        <p>
          Now in p(x) peaks are high-probability data (like clear images of cats) and the valleys are low-probability noise,
          the gradient of p(x) acts like a <strong>compass</strong>.
          So we use the score to "push" random noise toward the data peaks basically gradient ascent.
        </p>

        <p>
          Why log is just mathematical convenience cause prob at points might be very low like in orders of -50 then that exponential term removal & since logarithm is a <strong>monotonically increasing function</strong>.
          This means that the peak of p(x) is in the exact same location as the peak of log p(x).
        </p>

        <p>
          $$
          \nabla_x \log p(x)
          = \frac{\nabla_x p(x)}{p(x)}
          $$
        </p>

        <img src="../media/diffusion_101/download.png" class="rounded-lg shadow-md" />
      </section>

      <section class="space-y-4">
        <p>
          Langevin Diffusion describes a <strong>Stochastic Differential Equation</strong>.
        </p>

        <p>
          $$
          dX_t = \nabla\log p(X_t) + \sqrt{2}W_t
          $$
        </p>

        <p>
          $$
          X_{t+s} = X_{t}+s\nabla\log p(X_{t})+\sqrt{2s}\epsilon,
          \quad \epsilon\sim\mathcal{N}(0,\mathbf{I}_d)
          $$
        </p>

        <p>
          Intuitively, the Langevin diffusion is gradient ascent maximizing the log-probability injected with random Gaussian noise.
        </p>
      </section>

      <section class="space-y-4">
        <p>
          Now the main thing is that actually we don’t know gradient for the clean data so now we’ll train a neural network (the <strong>Score Estimator</strong>)
          to predict the gradient of the noisy data.
        </p>
      </section>

      <section class="space-y-4">
        <h2 class="font-bold text-xl">What the hell is this ItoSDE??</h2>

        <p>
          SDE describes how one particle moves, if you simulate 10,000 particles at once,
          the density of where those particles end up will eventually match the probability distribution p(x).
        </p>

        <img src="../media/diffusion_101/download 1.png" class="rounded-lg shadow-md" />
        <img src="../media/diffusion_101/download.png" class="rounded-lg shadow-md" />
      </section>

      <section class="space-y-4">
        <p>
          Brownian motion:
        </p>

        <p>
          $$
          W_{t+s} = W_{t} +\sqrt{s} \epsilon,\quad \epsilon\sim\mathcal{N}(0,1)
          $$
        </p>

        <p>
          ODE vs SDE:
        </p>

        <p>
          $$
          \frac{d}{dt}\mathbf{x}(t) = f(\mathbf{x}(t),t)
          $$
        </p>

        <p>
          $$
          dX(t) = f(X_t,t)dt+g(t)dW_t
          $$
        </p>
      </section>

      <section class="space-y-4">
        <p>
          $$
          X_{t+s}|X_t\sim\mathcal{N}(X_{t}+s f(X_t,t),sg^2(t))
          $$
        </p>

        <p>
          deterministic drift f describes the infinitesimal change in mean and the volatility function g describes the infinitesimal standard deviation.
        </p>
      </section>

      <section class="space-y-4">
        <p>
          affine drift assumption:
        </p>

        <p>
          $$
          f(x,t)=a(t)x+b(t)
          $$
        </p>

        <p>
          $$
          \mathbb{E}[X_t|X_0]
          =\left(X_0+\int\limits_{0}^{t}\exp(-\int\limits_{0}^{s}a(r)dr)b(s)ds\right)
          \exp\left(\int\limits_{0}^{t}a(s)ds\right)
          $$
        </p>
      </section>

      <section class="space-y-4">
        <p>
          There are 3 frameworks to build SDEs for diffusion models:
          variance-preserving, variance sub-preserving and variance-exploding.
        </p>
      </section>

      <section class="space-y-4">
        <p>
          variance exploding:
        </p>

        <p>
          $$
          \mathbb{E}[X_t|X_0]=X_0,\quad
          \mathbb{V}[X_t|X_0]=\beta(t)
          $$
        </p>

        <p>
          $$
          Y_t=\frac{X_t}{\sqrt{\beta(t)}}
          $$
        </p>

        <img src="../media/diffusion_101/variance_sde.png" class="rounded-lg shadow-md" />
      </section>

      <section class="space-y-4">
        <p>
          variance schedules:
        </p>

        <p>
          $$
          \sqrt{\beta(t)}=\sigma_{\text{min}}\left(\frac{\sigma_{\text{max}}}{\sigma_{\text{min}}}\right)^t
          $$
        </p>

        <p>
          $$
          \beta(t)=\frac{1}{2}t^2(\beta_{\text{max}}-\beta_{\text{min}})+t\beta_{\text{min}}
          $$
        </p>

        <img src="../media/diffusion_101/diffusion_variance.png" class="rounded-lg shadow-md" />
      </section>

      <section class="space-y-4">
        <p>
          reverse-time SDE:
        </p>

        <p>
          $$
          d\bar{X}_t= [f(X_t,t)-g^2(t)\frac{\partial}{\partial x}\log p_{t}(\bar{X}_t)]dt + g(t)d\bar{W}_t
          $$
        </p>

        <p>
          $$
          \bar{X}_{t-s} \approx \bar{X}_t +s\left[g^2(t)s_{\theta}(\bar{X}_t,t)-f(\bar{X}_t,t)\right]+\sqrt{s}g(t)\epsilon
          $$
        </p>
      </section>

      <section>
        <p>. . . to be continued</p>
      </section>

    </article>

    <script>
      lucide.createIcons();
    </script>
  </body>
</html>
